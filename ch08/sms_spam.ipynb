{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Collection Dataset\n",
    "\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.(https://www.kaggle.com/uciml/sms-spam-collection-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../datasets/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')\n",
    "dataset = dataset.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "dataset = dataset.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "      count\n",
       "label      \n",
       "ham    4825\n",
       "spam    747"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby([\"label\"]).agg([\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning text data\n",
    "    - user lowercase\n",
    "    - remove non-word characters (excluding emoticons)\n",
    "    - remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub(\"[\\W\\d]+\", \" \", text.lower()) + ' '.join(emoticons).replace(\"-\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"text\"] = dataset[\"text\"].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_stemmer(stemmer):\n",
    "    def tokenizer(text):\n",
    "        return [ stemmer.stem(word) for word in text.split() ]\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer_porter = tokenizer_stemmer(PorterStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'ride', 'my', 'bike']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_porter(\"I am riding my bike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.label.map({\"ham\": 0, \"spam\": 1})\n",
    "X = dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample:\n",
      " 0    0.865897\n",
      "1    0.134103\n",
      "Name: label, dtype: float64\n",
      "Test sample:\n",
      " 0    0.866029\n",
      "1    0.133971\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training sample:\\n\", pd.Series(y_train).value_counts(normalize=True))\n",
    "print(\"Test sample:\\n\", pd.Series(y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tfidf = Pipeline([\n",
    "    (\"vect\", TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range': [(1, 1), (2, 2)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': [1.0, 10.0, 100.0]\n",
    "    },\n",
    "#     {\n",
    "#         'vect__ngram_range': [(1, 1), (2, 2)],\n",
    "#         'vect__stop_words': [stop, None],\n",
    "#         'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "#         'vect__use_idf':[False],\n",
    "#         'vect__norm':[None],\n",
    "#         'clf__penalty': ['l1', 'l2'],\n",
    "#         'clf__C': [1.0, 10.0, 100.0]\n",
    "#     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1), (2, 2)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it...<locals>.tokenizer at 0x7fe8d3965158>], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring=\"accuracy\", cv=5, verbose=1, n_jobs=1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 100.0,\n",
       " 'clf__penalty': 'l2',\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <function __main__.tokenizer_stemmer.<locals>.tokenizer>}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.986\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                                                  ham\n",
       "text     hi the way i was with u day is the normal way ...\n",
       "Name: 313, dtype: object"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[313,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words=\"english\", ngram_range=(2, 2)).fit(dataset[\"text\"])\n",
    "bag = count.transform(dataset[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['____ joy',\n",
       " 'aa exhaust',\n",
       " 'aah bless',\n",
       " 'aah cuddle',\n",
       " 'aah speak',\n",
       " 'aaniye pudunga',\n",
       " 'aaooooright work',\n",
       " 'aathi dear',\n",
       " 'aathi love',\n",
       " 'ab sara',\n",
       " 'abbey happy',\n",
       " 'abdomen gynae',\n",
       " 'abeg make',\n",
       " 'aberdeen united',\n",
       " 'abi hw',\n",
       " 'abi just',\n",
       " 'ability listen',\n",
       " 'ability question',\n",
       " 'abj serving',\n",
       " 'able atten',\n",
       " 'able buy',\n",
       " 'able come',\n",
       " 'able deliver',\n",
       " 'able dont',\n",
       " 'able eat',\n",
       " 'able friday',\n",
       " 'able half',\n",
       " 'able join',\n",
       " 'able kids',\n",
       " 'able late',\n",
       " 'able little',\n",
       " 'able ll',\n",
       " 'able met',\n",
       " 'able morning',\n",
       " 'able ors',\n",
       " 'able pay',\n",
       " 'able raise',\n",
       " 'able reply',\n",
       " 'able shopping',\n",
       " 'able sleep',\n",
       " 'able value',\n",
       " 'abnormally ll',\n",
       " 'aboutas chance',\n",
       " 'abroad lonely',\n",
       " 'absence gud',\n",
       " 'absolutely love',\n",
       " 'absolutly fine',\n",
       " 'abstract wake',\n",
       " 'abt abt',\n",
       " 'abt character',\n",
       " 'abt concentrate',\n",
       " 'abt dvg',\n",
       " 'abt events',\n",
       " 'abt functions',\n",
       " 'abt half',\n",
       " 'abt leona',\n",
       " 'abt making',\n",
       " 'abt mei',\n",
       " 'abt movie',\n",
       " 'abt muz',\n",
       " 'abt rite',\n",
       " 'abt rows',\n",
       " 'abt syd',\n",
       " 'abt tat',\n",
       " 'abt tel',\n",
       " 'abt tht',\n",
       " 'abt trip',\n",
       " 'abt ur',\n",
       " 'abt whr',\n",
       " 'abta complimentary',\n",
       " 'aburo enjoy',\n",
       " 'abusers lives',\n",
       " 'ac blind',\n",
       " 'ac bootydelious',\n",
       " 'ac goldviking',\n",
       " 'ac hmmross',\n",
       " 'ac icmb',\n",
       " 'ac jsco',\n",
       " 'ac nat',\n",
       " 'ac natalie',\n",
       " 'ac smsrewards',\n",
       " 'ac sptv',\n",
       " 'ac sun',\n",
       " 'academic department',\n",
       " 'academic secretary',\n",
       " 'acc bank',\n",
       " 'acc details',\n",
       " 'acc wen',\n",
       " 'accent important',\n",
       " 'accenture confirm',\n",
       " 'accept brother',\n",
       " 'accept day',\n",
       " 'accept life',\n",
       " 'accept single',\n",
       " 'access adult',\n",
       " 'access games',\n",
       " 'access hrs',\n",
       " 'access number',\n",
       " 'access til',\n",
       " 'access www',\n",
       " 'accessible just',\n",
       " 'accidant tookplace',\n",
       " 'accident claim',\n",
       " 'accident divert',\n",
       " 'accidentally brought',\n",
       " 'accidentally deleted',\n",
       " 'accidentally left',\n",
       " 'accommodation various',\n",
       " 'accommodationvouchers terms',\n",
       " 'accomodations thought',\n",
       " 'accordin wat',\n",
       " 'accordingly ok',\n",
       " 'accordingly repeat',\n",
       " 'accordingly tell',\n",
       " 'account balance',\n",
       " 'account bank',\n",
       " 'account credited',\n",
       " 'account details',\n",
       " 'account dry',\n",
       " 'account hr',\n",
       " 'account identification',\n",
       " 'account just',\n",
       " 'account lt',\n",
       " 'account number',\n",
       " 'account refilled',\n",
       " 'account statement',\n",
       " 'account thanks',\n",
       " 'account xxxxxxxxx',\n",
       " 'accounting delayed',\n",
       " 'accounts executive',\n",
       " 'achan amma',\n",
       " 'ache don',\n",
       " 'ache feel',\n",
       " 'ache fuck',\n",
       " 'ache speak',\n",
       " 'achieve korli',\n",
       " 'acknowledgement astoundingly',\n",
       " 'acl pm',\n",
       " 'aco entry',\n",
       " 'act real',\n",
       " 'acted like',\n",
       " 'actin like',\n",
       " 'acting hear',\n",
       " 'action real',\n",
       " 'action txt',\n",
       " 'action www',\n",
       " 'activ press',\n",
       " 'activate free',\n",
       " 'activate just',\n",
       " 'activate smartcall',\n",
       " 'activate version',\n",
       " 'activate wire',\n",
       " 'active play',\n",
       " 'activities planned',\n",
       " 'actor work',\n",
       " 'actual exam',\n",
       " 'actual guy',\n",
       " 'actually awake',\n",
       " 'actually better',\n",
       " 'actually born',\n",
       " 'actually called',\n",
       " 'actually decided',\n",
       " 'actually deleted',\n",
       " 'actually did',\n",
       " 'actually flies',\n",
       " 'actually fuck',\n",
       " 'actually getting',\n",
       " 'actually got',\n",
       " 'actually guys',\n",
       " 'actually life',\n",
       " 'actually little',\n",
       " 'actually lt',\n",
       " 'actually mobile',\n",
       " 'actually necessity',\n",
       " 'actually nvm',\n",
       " 'actually pretty',\n",
       " 'actually quite',\n",
       " 'actually reasonable',\n",
       " 'actually send',\n",
       " 'actually sleeping',\n",
       " 'actually start',\n",
       " 'actually styling',\n",
       " 'actually talk',\n",
       " 'actually thk',\n",
       " 'actually tonight',\n",
       " 'actually waiting',\n",
       " 'ad craigslist',\n",
       " 'ad crap',\n",
       " 'ad nice',\n",
       " 'ad row',\n",
       " 'ad trueåác',\n",
       " 'adam eve',\n",
       " 'add don',\n",
       " 'add figures',\n",
       " 'add spice',\n",
       " 'add tat',\n",
       " 'add ur',\n",
       " 'add years',\n",
       " 'add ì_',\n",
       " 'addamsfa munsters',\n",
       " 'added benefits',\n",
       " 'added contact',\n",
       " 'addicted msging',\n",
       " 'addicted sweet',\n",
       " 'addie amp',\n",
       " 'addie goes',\n",
       " 'adding zeros',\n",
       " 'address apples',\n",
       " 'address boss',\n",
       " 'address carlos',\n",
       " 'address changed',\n",
       " 'address dob',\n",
       " 'address envelope',\n",
       " 'address icky',\n",
       " 'address incomm',\n",
       " 'address ll',\n",
       " 'address lt',\n",
       " 'address occurs',\n",
       " 'address receive',\n",
       " 'address sandiago',\n",
       " 'address sir',\n",
       " 'address test',\n",
       " 'adds life',\n",
       " 'adewale uncle',\n",
       " 'adi entey',\n",
       " 'adjustable cooperative',\n",
       " 'admin building',\n",
       " 'admin team',\n",
       " 'administrator network',\n",
       " 'admirer looking',\n",
       " 'admirer reveal',\n",
       " 'admission til',\n",
       " 'admit mad',\n",
       " 'admit wrong',\n",
       " 'adore ahmad',\n",
       " 'adore loverboy',\n",
       " 'adoring kiss',\n",
       " 'adrian text',\n",
       " 'adsense approved',\n",
       " 'adult content',\n",
       " 'adult learn',\n",
       " 'adult parties',\n",
       " 'adult singles',\n",
       " 'adults just',\n",
       " 'advance haha',\n",
       " 'advance happy',\n",
       " 'advance just',\n",
       " 'adventure galileo',\n",
       " 'advice come',\n",
       " 'advice enjoy',\n",
       " 'advice gary',\n",
       " 'advice thanks',\n",
       " 'advise following',\n",
       " 'advisors free',\n",
       " 'aeronautics professors',\n",
       " 'aeroplane aftr',\n",
       " 'afew days',\n",
       " 'affair cheers',\n",
       " 'affairs ask',\n",
       " 'affairs come',\n",
       " 'affairs wif',\n",
       " 'affection care',\n",
       " 'affections amp',\n",
       " 'affidavit says',\n",
       " 'afford tells',\n",
       " 'afghanistan stable',\n",
       " 'afraid dark',\n",
       " 'afraid staying',\n",
       " 'africa hope',\n",
       " 'african soil',\n",
       " 'aft ask',\n",
       " 'aft bathing',\n",
       " 'aft come',\n",
       " 'aft dat',\n",
       " 'aft finish',\n",
       " 'aft guesses',\n",
       " 'aft lunch',\n",
       " 'aft meeting',\n",
       " 'aft thinking',\n",
       " 'aft tmr',\n",
       " 'aft toa',\n",
       " 'aft ur',\n",
       " 'aft wan',\n",
       " 'aft wat',\n",
       " 'afternon love',\n",
       " 'afternoon babe',\n",
       " 'afternoon boytoy',\n",
       " 'afternoon brings',\n",
       " 'afternoon casualty',\n",
       " 'afternoon finish',\n",
       " 'afternoon glorious',\n",
       " 'afternoon love',\n",
       " 'afternoon loverboy',\n",
       " 'afternoon mah',\n",
       " 'afternoon onwords',\n",
       " 'afternoon sexy',\n",
       " 'afternoon starshine',\n",
       " 'afternoon sunshine',\n",
       " 'afternoon sup',\n",
       " 'afternoon thought',\n",
       " 'afternoon town',\n",
       " 'afternoon watching',\n",
       " 'afternoon wife',\n",
       " 'afternoons evenings',\n",
       " 'aftr decades',\n",
       " 'aftr lt',\n",
       " 'aftr pm',\n",
       " 'aftr sat',\n",
       " 'ag promo',\n",
       " 'agalla boy',\n",
       " 'age blonde',\n",
       " 'age deciding',\n",
       " 'age followed',\n",
       " 'age gender',\n",
       " 'age girl',\n",
       " 'age join',\n",
       " 'age opt',\n",
       " 'age perweeksub',\n",
       " 'age perwksub',\n",
       " 'age ppermesssubscription',\n",
       " 'age range',\n",
       " 'age sam',\n",
       " 'age stop',\n",
       " 'age stoptxtstopå',\n",
       " 'age subscription',\n",
       " 'age supposed',\n",
       " 'age verify',\n",
       " 'agency renting',\n",
       " 'agent mob',\n",
       " 'agents don',\n",
       " 'ages abj',\n",
       " 'ages ring',\n",
       " 'ages started',\n",
       " 'ages yeah',\n",
       " 'aging products',\n",
       " 'ago cusoon',\n",
       " 'ago dunno',\n",
       " 'ago guy',\n",
       " 'ago liao',\n",
       " 'ago showered',\n",
       " 'ago wat',\n",
       " 'ago wtf',\n",
       " 'agree price',\n",
       " 'agree stop',\n",
       " 'ah confuses',\n",
       " 'ah coz',\n",
       " 'ah den',\n",
       " 'ah dun',\n",
       " 'ah failed',\n",
       " 'ah gee',\n",
       " 'ah lingo',\n",
       " 'ah machi',\n",
       " 'ah meet',\n",
       " 'ah muz',\n",
       " 'ah okie',\n",
       " 'ah opps',\n",
       " 'ah poop',\n",
       " 'ah poor',\n",
       " 'ah said',\n",
       " 'ah sat',\n",
       " 'ah sen',\n",
       " 'ah thk',\n",
       " 'ah tmr',\n",
       " 'ah ubi',\n",
       " 'ah waitin',\n",
       " 'ah wat',\n",
       " 'ah ì_',\n",
       " 'aha da',\n",
       " 'ahead amp',\n",
       " 'ahead month',\n",
       " 'ahead smoke',\n",
       " 'ahead watts',\n",
       " 'ahhh work',\n",
       " 'ahhhh just',\n",
       " 'ahmad adoring',\n",
       " 'ahmad al',\n",
       " 'ahmad kisses',\n",
       " 'ahmad saeed',\n",
       " 'ahmad wait',\n",
       " 'ahold anybody',\n",
       " 'aid usmle',\n",
       " 'aids patent',\n",
       " 'aig joined',\n",
       " 'aight bit',\n",
       " 'aight chillin',\n",
       " 'aight close',\n",
       " 'aight fuck',\n",
       " 'aight ill',\n",
       " 'aight informed',\n",
       " 'aight just',\n",
       " 'aight latest',\n",
       " 'aight lemme',\n",
       " 'aight let',\n",
       " 'aight ll',\n",
       " 'aight pick',\n",
       " 'aight rush',\n",
       " 'aight sorry',\n",
       " 'aight sounds',\n",
       " 'aight text',\n",
       " 'aight thanks',\n",
       " 'aight time',\n",
       " 'aight tomorrow',\n",
       " 'aight ve',\n",
       " 'aight want',\n",
       " 'aight wat',\n",
       " 'aight yo',\n",
       " 'ain answerin',\n",
       " 'ain gonna',\n",
       " 'ain smokin',\n",
       " 'ain tobacco',\n",
       " 'aint bad',\n",
       " 'aint coming',\n",
       " 'aint complaining',\n",
       " 'aint thing',\n",
       " 'aint tho',\n",
       " 'air balloon',\n",
       " 'air force',\n",
       " 'air hilarious',\n",
       " 'air know',\n",
       " 'air opinions',\n",
       " 'air smile',\n",
       " 'air talent',\n",
       " 'airport evening',\n",
       " 'airport lounge',\n",
       " 'airport rd',\n",
       " 'airport road',\n",
       " 'airport satsgettin',\n",
       " 'airtel broadband',\n",
       " 'airtel line',\n",
       " 'aiya discuss',\n",
       " 'aiya dunno',\n",
       " 'aiya later',\n",
       " 'aiya thk',\n",
       " 'aiyah did',\n",
       " 'aiyah ok',\n",
       " 'aiyah rain',\n",
       " 'aiyah sorry',\n",
       " 'aiyah tok',\n",
       " 'aiyah wait',\n",
       " 'aiyar dun',\n",
       " 'aiyar hard',\n",
       " 'aiyar poor',\n",
       " 'aiyar sorry',\n",
       " 'aiyo bit',\n",
       " 'aiyo cos',\n",
       " 'aiyo ex',\n",
       " 'aiyo lesson',\n",
       " 'aiyo poor',\n",
       " 'aiyo ì_',\n",
       " 'ajith film',\n",
       " 'ak try',\n",
       " 'aka egbon',\n",
       " 'aka pap',\n",
       " 'akon lonely',\n",
       " 'al arguments',\n",
       " 'al does',\n",
       " 'al hallaq',\n",
       " 'al lt',\n",
       " 'al post',\n",
       " 'al salam',\n",
       " 'al sudn',\n",
       " 'alaikkum pride',\n",
       " 'alaipayuthe set',\n",
       " 'albi mufti',\n",
       " 'album quite',\n",
       " 'alcohol jay',\n",
       " 'aldrine rakhesh',\n",
       " 'alert computer',\n",
       " 'alert matches',\n",
       " 'alert messages',\n",
       " 'alert numbers',\n",
       " 'alertfrom jeri',\n",
       " 'alerts reply',\n",
       " 'aletter thatmum',\n",
       " 'alex knows',\n",
       " 'alex nichols',\n",
       " 'alex pizza',\n",
       " 'alex place',\n",
       " 'alex says',\n",
       " 'alfie moon',\n",
       " 'algarve txt',\n",
       " 'algebra today',\n",
       " 'algorithms second',\n",
       " 'ali halla',\n",
       " 'ali vargu',\n",
       " 'alibi cutting',\n",
       " 'alive better',\n",
       " 'alive breathe',\n",
       " 'alive gud',\n",
       " 'allah meet',\n",
       " 'allah rakhesh',\n",
       " 'allah tohar',\n",
       " 'allday hope',\n",
       " 'alle mone',\n",
       " 'allo braved',\n",
       " 'allow companies',\n",
       " 'allow work',\n",
       " 'allowed play',\n",
       " 'allows wife',\n",
       " 'alot wiv',\n",
       " 'alright babe',\n",
       " 'alright bit',\n",
       " 'alright did',\n",
       " 'alright fone',\n",
       " 'alright good',\n",
       " 'alright hooked',\n",
       " 'alright hows',\n",
       " 'alright ll',\n",
       " 'alright new',\n",
       " 'alright okay',\n",
       " 'alright omw',\n",
       " 'alright phews',\n",
       " 'alright set',\n",
       " 'alright sure',\n",
       " 'alright thanks',\n",
       " 'alright think',\n",
       " 'alright thinkin',\n",
       " 'alright took',\n",
       " 'alright tyler',\n",
       " 'alright way',\n",
       " 'alrite girl',\n",
       " 'alrite good',\n",
       " 'alrite hunny',\n",
       " 'alrite idiot',\n",
       " 'alrite jod',\n",
       " 'alrite sam',\n",
       " 'alter ok',\n",
       " 'alternative hope',\n",
       " 'alto uk',\n",
       " 'aluable ffectionate',\n",
       " 'alwa gud',\n",
       " 'alwys stay',\n",
       " 'alwys touch',\n",
       " 'amanda regard',\n",
       " 'amazing quote',\n",
       " 'amazing rearrange',\n",
       " 'amazing things',\n",
       " 'amazing xxx',\n",
       " 'ambrith madurai',\n",
       " 'american customer',\n",
       " 'american freek',\n",
       " 'american pie',\n",
       " 'ami parchi',\n",
       " 'amigos hoping',\n",
       " 'amk need',\n",
       " 'amla home',\n",
       " 'amma rakhesh',\n",
       " 'ammae life',\n",
       " 'ammo want',\n",
       " 'amore wat',\n",
       " 'amp answered',\n",
       " 'amp asks',\n",
       " 'amp best',\n",
       " 'amp care',\n",
       " 'amp church',\n",
       " 'amp closer',\n",
       " 'amp concern',\n",
       " 'amp conducts',\n",
       " 'amp doing',\n",
       " 'amp don',\n",
       " 'amp dont',\n",
       " 'amp drive',\n",
       " 'amp eve',\n",
       " 'amp express',\n",
       " 'amp fills',\n",
       " 'amp finally',\n",
       " 'amp fletcher',\n",
       " 'amp fowler',\n",
       " 'amp fuelled',\n",
       " 'amp happy',\n",
       " 'amp imf',\n",
       " 'amp irritated',\n",
       " 'amp kills',\n",
       " 'amp let',\n",
       " 'amp life',\n",
       " 'amp lt',\n",
       " 'amp miracle',\n",
       " 'amp miss',\n",
       " 'amp mobile',\n",
       " 'amp need',\n",
       " 'amp nice',\n",
       " 'amp packing',\n",
       " 'amp pain',\n",
       " 'amp pass',\n",
       " 'amp present',\n",
       " 'amp promise',\n",
       " 'amp reach',\n",
       " 'amp sack',\n",
       " 'amp said',\n",
       " 'amp say',\n",
       " 'amp sayin',\n",
       " 'amp send',\n",
       " 'amp smiley',\n",
       " 'amp step',\n",
       " 'amp successful',\n",
       " 'amp sweet',\n",
       " 'amp taylor',\n",
       " 'amp teaches',\n",
       " 'amp tht',\n",
       " 'amp traditions',\n",
       " 'amp wer',\n",
       " 'amp wish',\n",
       " 'amplikater fidalfication',\n",
       " 'amrca thing',\n",
       " 'amrita college',\n",
       " 'ams xx',\n",
       " 'amt lt',\n",
       " 'amused joking',\n",
       " 'amused just',\n",
       " 'amy sending',\n",
       " 'amy ure',\n",
       " 'ana tomarrow',\n",
       " 'anal sex',\n",
       " 'analysis starts',\n",
       " 'anand number',\n",
       " 'andre virgil',\n",
       " 'andres money',\n",
       " 'andrews boy',\n",
       " 'andros ice',\n",
       " 'andros steal',\n",
       " 'angels snowball',\n",
       " 'angry busy',\n",
       " 'angry childish',\n",
       " 'angry happen',\n",
       " 'angry left',\n",
       " 'angry misbehaved',\n",
       " 'angry msg',\n",
       " 'angry practice',\n",
       " 'angry reply',\n",
       " 'angry told',\n",
       " 'angry wid',\n",
       " 'animal just',\n",
       " 'animation badass',\n",
       " 'anna nagar',\n",
       " 'anniversary day',\n",
       " 'anniversary party',\n",
       " 'annoncement new',\n",
       " 'announced blog',\n",
       " 'announcement freephone',\n",
       " 'announcement premier',\n",
       " 'announcement recently',\n",
       " 'annoying day',\n",
       " 'annoying isn',\n",
       " 'anonymous masked',\n",
       " 'anot thk',\n",
       " 'ans bslvyl',\n",
       " 'ans elvis',\n",
       " 'ans holiday',\n",
       " 'ans lar',\n",
       " 'ans olympics',\n",
       " 'ans question',\n",
       " 'ans ths',\n",
       " 'ans whats',\n",
       " 'ansr sp',\n",
       " 'answer asked',\n",
       " 'answer assume',\n",
       " 'answer brilliant',\n",
       " 'answer den',\n",
       " 'answer easy',\n",
       " 'answer fucking',\n",
       " 'answer good',\n",
       " 'answer know',\n",
       " 'answer question',\n",
       " 'answer questions',\n",
       " 'answer texts',\n",
       " 'answer txt',\n",
       " 'answered fine',\n",
       " 'answerin phone',\n",
       " 'answering everyones',\n",
       " 'answering maybe',\n",
       " 'answering phone',\n",
       " 'answering texts',\n",
       " 'answers fighting',\n",
       " 'answers pls',\n",
       " 'answers really',\n",
       " 'antelope begin',\n",
       " 'antha num',\n",
       " 'anthony bringing',\n",
       " 'anthony dad',\n",
       " 'anti aging',\n",
       " 'anti sleep',\n",
       " 'anti social',\n",
       " 'antibiotic used',\n",
       " 'anybody asks',\n",
       " 'anybody chip',\n",
       " 'anybody let',\n",
       " 'anybody number',\n",
       " 'anymore jia',\n",
       " 'anymore like',\n",
       " 'anymore say',\n",
       " 'anymore tonight',\n",
       " 'anyones help',\n",
       " 'anyplaces doing',\n",
       " 'anythiing keeping',\n",
       " 'anythin likes',\n",
       " 'anythin special',\n",
       " 'anythin thk',\n",
       " 'anythingtomorrow myparents',\n",
       " 'anytime best',\n",
       " 'anytime calling',\n",
       " 'anytime lor',\n",
       " 'anytime network',\n",
       " 'anytime networks',\n",
       " 'anyways far',\n",
       " 'anyways gr',\n",
       " 'anyways great',\n",
       " 'anyways just',\n",
       " 'aom box',\n",
       " 'aom just',\n",
       " 'apart things',\n",
       " 'apart told',\n",
       " 'apartment chennai',\n",
       " 'apartment got',\n",
       " 'apartment went',\n",
       " 'apes fight',\n",
       " 'aphexåõs abel',\n",
       " 'apnt pm',\n",
       " 'apo good',\n",
       " 'apo mokka',\n",
       " 'apologetic fallen',\n",
       " 'apologise advance',\n",
       " 'apologise cali',\n",
       " 'apologize admit',\n",
       " 'apology texting',\n",
       " 'app phone',\n",
       " 'apparently bffs',\n",
       " 'apparently drive',\n",
       " 'apparently happens',\n",
       " 'apparently retired',\n",
       " 'appeal tomo',\n",
       " 'appear forget',\n",
       " 'appendix age',\n",
       " 'applausestore com',\n",
       " 'apple day',\n",
       " 'applebees fucking',\n",
       " 'apples pairs',\n",
       " 'application airtel',\n",
       " 'application schools',\n",
       " 'apply ag',\n",
       " 'apply claim',\n",
       " 'apply details',\n",
       " 'apply enjoy',\n",
       " 'apply future',\n",
       " 'apply need',\n",
       " 'apply opt',\n",
       " 'apply phd',\n",
       " 'apply reply',\n",
       " 'apply stop',\n",
       " 'apply www',\n",
       " 'apply yrs',\n",
       " 'applyed oil',\n",
       " 'applying research',\n",
       " 'appointment ask',\n",
       " 'appointment need',\n",
       " 'appointment right',\n",
       " 'appointments week',\n",
       " 'appreciate care',\n",
       " 'appreciate just',\n",
       " 'appreciate keeping',\n",
       " 'appreciate sacrifice',\n",
       " 'appreciate safe',\n",
       " 'appreciate tomorrow',\n",
       " 'appreciated dad',\n",
       " 'appreciated yeah',\n",
       " 'approaches hurts',\n",
       " 'approaching wish',\n",
       " 'approve panalam',\n",
       " 'approx mins',\n",
       " 'approx month',\n",
       " 'apps class',\n",
       " 'apps da',\n",
       " 'appt lt',\n",
       " 'appt shame',\n",
       " 'appt week',\n",
       " 'appy fizz',\n",
       " 'april audition',\n",
       " 'april les',\n",
       " 'april ll',\n",
       " 'april real',\n",
       " 'april wait',\n",
       " 'aproach gal',\n",
       " 'apt opportunity',\n",
       " 'aptitude class',\n",
       " 'aq ppm',\n",
       " 'ar married',\n",
       " 'ar praveesh',\n",
       " 'ar sweet',\n",
       " 'arab boy',\n",
       " 'arabian steed',\n",
       " 'arcade game',\n",
       " 'arcade st',\n",
       " 'arcade std',\n",
       " 'arcade termsapply',\n",
       " 'archive haiz',\n",
       " 'ard christmas',\n",
       " 'ard like',\n",
       " 'ard lor',\n",
       " 'ard mins',\n",
       " 'ard need',\n",
       " 'ard oso',\n",
       " 'ard pages',\n",
       " 'ard price',\n",
       " 'ard smth',\n",
       " 'ard wan',\n",
       " 'ard wat',\n",
       " 'ard wun',\n",
       " 'ard ya',\n",
       " 'ard ìä',\n",
       " 'area forgot',\n",
       " 'area join',\n",
       " 'area just',\n",
       " 'area lots',\n",
       " 'area matched',\n",
       " 'area real',\n",
       " 'area reply',\n",
       " 'area restaurant',\n",
       " 'area thing',\n",
       " 'area urgnt',\n",
       " 'aren bookshelf',\n",
       " 'aren coming',\n",
       " 'aren lt',\n",
       " 'aren waiting',\n",
       " 'arent bright',\n",
       " 'arent hot',\n",
       " 'arestaurant eating',\n",
       " 'aretaking outfor',\n",
       " 'areyouunique uk',\n",
       " 'argentina sad',\n",
       " 'argh fuck',\n",
       " 'argh ok',\n",
       " 'argh spotty',\n",
       " 'argue treats',\n",
       " 'argue ur',\n",
       " 'arguing week',\n",
       " 'argument apart',\n",
       " 'argument week',\n",
       " 'argument wins',\n",
       " 'arguments fault',\n",
       " 'arise hurricanes',\n",
       " 'arises hell',\n",
       " 'arithmetic percentages',\n",
       " 'arm feeling',\n",
       " 'armand eventually',\n",
       " 'armand really',\n",
       " 'armand says',\n",
       " 'armand shit',\n",
       " 'armenia ends',\n",
       " 'arms fine',\n",
       " 'arms minutes',\n",
       " 'arms right',\n",
       " 'arng dis',\n",
       " 'arngd marriage',\n",
       " 'arnt pissed',\n",
       " 'aroundn teaches',\n",
       " 'arr birthday',\n",
       " 'arrange delivery',\n",
       " 'arrange evening',\n",
       " 'arrange pick',\n",
       " 'arrange shipping',\n",
       " 'arrange transport',\n",
       " 'arranging time',\n",
       " 'arrested day',\n",
       " 'arrested murderer',\n",
       " 'arrested possession',\n",
       " 'arrival time',\n",
       " 'arrive customer',\n",
       " 'arrive shortly',\n",
       " 'arrived couple',\n",
       " 'arsenal dartboard',\n",
       " 'arsenal goal',\n",
       " 'arsenal henry',\n",
       " 'art class',\n",
       " 'art ll',\n",
       " 'arts college',\n",
       " 'arts today',\n",
       " 'arty collages',\n",
       " 'arul lt',\n",
       " 'arun dha',\n",
       " 'arun transfr',\n",
       " 'asa ll',\n",
       " 'asap box',\n",
       " 'asap ok',\n",
       " 'asap ta',\n",
       " 'asap try',\n",
       " 'asap xxx',\n",
       " 'asda counts',\n",
       " 'ashley number',\n",
       " 'ashwini ur',\n",
       " 'asia expensive',\n",
       " 'asian tsunami',\n",
       " 'asjesus com',\n",
       " 'ask abt',\n",
       " 'ask ah',\n",
       " 'ask alex',\n",
       " 'ask ask',\n",
       " 'ask asking',\n",
       " 'ask big',\n",
       " 'ask bit',\n",
       " 'ask bout',\n",
       " 'ask bring',\n",
       " 'ask brother',\n",
       " 'ask carlos',\n",
       " 'ask chechi',\n",
       " 'ask check',\n",
       " 'ask come',\n",
       " 'ask confirm',\n",
       " 'ask connect',\n",
       " 'ask cooked',\n",
       " 'ask cos',\n",
       " 'ask crazy',\n",
       " 'ask created',\n",
       " 'ask cut',\n",
       " 'ask da',\n",
       " 'ask dad',\n",
       " 'ask darren',\n",
       " 'ask different',\n",
       " 'ask especially',\n",
       " 'ask fri',\n",
       " 'ask friendship',\n",
       " 'ask gone',\n",
       " 'ask happened',\n",
       " 'ask helpful',\n",
       " 'ask iouri',\n",
       " 'ask jay',\n",
       " 'ask join',\n",
       " 'ask ll',\n",
       " 'ask lol',\n",
       " 'ask lot',\n",
       " 'ask luck',\n",
       " 'ask macho',\n",
       " 'ask maybe',\n",
       " 'ask meeting',\n",
       " 'ask miwa',\n",
       " 'ask mom',\n",
       " 'ask mummy',\n",
       " 'ask nickey',\n",
       " 'ask pa',\n",
       " 'ask princess',\n",
       " 'ask questions',\n",
       " 'ask roommates',\n",
       " 'ask said',\n",
       " 'ask sat',\n",
       " 'ask say',\n",
       " 'ask sell',\n",
       " 'ask shuhui',\n",
       " 'ask speed',\n",
       " 'ask st',\n",
       " 'ask suitemates',\n",
       " 'ask superior',\n",
       " 'ask tell',\n",
       " 'ask thing',\n",
       " 'ask tmr',\n",
       " 'ask today',\n",
       " 'ask transfer',\n",
       " 'ask vava',\n",
       " 'ask wana',\n",
       " 'ask wat',\n",
       " 'ask workin',\n",
       " 'ask working',\n",
       " 'ask ì_',\n",
       " 'askd did',\n",
       " 'askd lunch',\n",
       " 'askd question',\n",
       " 'askd sit',\n",
       " 'asked anand',\n",
       " 'asked anthony',\n",
       " 'asked boy',\n",
       " 'asked come',\n",
       " 'asked contact',\n",
       " 'asked dating',\n",
       " 'asked dubsack',\n",
       " 'asked forgiveness',\n",
       " 'asked frnd',\n",
       " 'asked fun',\n",
       " 'asked girl',\n",
       " 'asked hw',\n",
       " 'asked mobile',\n",
       " 'asked money',\n",
       " 'asked needed',\n",
       " 'asked ok',\n",
       " 'asked radio',\n",
       " 'asked sen',\n",
       " 'asked waheeda',\n",
       " 'asked wanted',\n",
       " 'askin ahead',\n",
       " 'askin cos',\n",
       " 'askin lt',\n",
       " 'askin things',\n",
       " 'askin ì_',\n",
       " 'asking gym',\n",
       " 'asking knw',\n",
       " 'asking like',\n",
       " 'asking missed',\n",
       " 'asking practicum',\n",
       " 'asking shuhui',\n",
       " 'asking slippers',\n",
       " 'asking stuff',\n",
       " 'asking tiwary',\n",
       " 'asking wats',\n",
       " 'asking working',\n",
       " ...]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm=\"l2\", smooth_idf=True)\n",
    "tfidf.fit(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.44967417, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.47902097, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.48469055, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.57741417, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform(bag).toarray()[2,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point, crazy.. Available only ...\n",
       "1                           Ok lar... Joking wif u oni...\n",
       "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       U dun say so early hor... U c already then say...\n",
       "4       Nah I don't think he goes to usf, he lives aro...\n",
       "5       FreeMsg Hey there darling it's been 3 week's n...\n",
       "6       Even my brother is not like to speak with me. ...\n",
       "7       As per your request 'Melle Melle (Oru Minnamin...\n",
       "8       WINNER!! As a valued network customer you have...\n",
       "9       Had your mobile 11 months or more? U R entitle...\n",
       "10      I'm gonna be home soon and i don't want to tal...\n",
       "11      SIX chances to win CASH! From 100 to 20,000 po...\n",
       "12      URGENT! You have won a 1 week FREE membership ...\n",
       "13      I've been searching for the right words to tha...\n",
       "14                    I HAVE A DATE ON SUNDAY WITH WILL!!\n",
       "15      XXXMobileMovieClub: To use your credit, click ...\n",
       "16                             Oh k...i'm watching here:)\n",
       "17      Eh u remember how 2 spell his name... Yes i di...\n",
       "18      Fine if thatåÕs the way u feel. ThatåÕs the wa...\n",
       "19      England v Macedonia - dont miss the goals/team...\n",
       "20              Is that seriously how you spell his name?\n",
       "21      IÛ÷m going to try for 2 months ha ha only joking\n",
       "22      So Ì_ pay first lar... Then when is da stock c...\n",
       "23      Aft i finish my lunch then i go str down lor. ...\n",
       "24      Ffffffffff. Alright no way I can meet up with ...\n",
       "25      Just forced myself to eat a slice. I'm really ...\n",
       "26                         Lol your always so convincing.\n",
       "27      Did you catch the bus ? Are you frying an egg ...\n",
       "28      I'm back &amp; we're packing the car now, I'll...\n",
       "29      Ahhh. Work. I vaguely remember that! What does...\n",
       "                              ...                        \n",
       "5542             Armand says get your ass over to epsilon\n",
       "5543               U still havent got urself a jacket ah?\n",
       "5544    I'm taking derek &amp; taylor to walmart, if I...\n",
       "5545        Hi its in durban are you still on this number\n",
       "5546           Ic. There are a lotta childporn cars then.\n",
       "5547    Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5548                   No, I was trying it all weekend ;V\n",
       "5549    You know, wot people wear. T shirts, jumpers, ...\n",
       "5550          Cool, what time you think you can get here?\n",
       "5551    Wen did you get so spiritual and deep. That's ...\n",
       "5552    Have a safe trip to Nigeria. Wish you happines...\n",
       "5553                          Hahaha..use your brain dear\n",
       "5554    Well keep in mind I've only got enough gas for...\n",
       "5555    Yeh. Indians was nice. Tho it did kane me off ...\n",
       "5556    Yes i have. So that's why u texted. Pshew...mi...\n",
       "5557    No. I meant the calculation is the same. That ...\n",
       "5558                               Sorry, I'll call later\n",
       "5559    if you aren't here in the next  &lt;#&gt;  hou...\n",
       "5560                    Anything lor. Juz both of us lor.\n",
       "5561    Get me out of this dump heap. My mom decided t...\n",
       "5562    Ok lor... Sony ericsson salesman... I ask shuh...\n",
       "5563                                  Ard 6 like dat lor.\n",
       "5564    Why don't you wait 'til at least wednesday to ...\n",
       "5565                                         Huh y lei...\n",
       "5566    REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567    This is the 2nd time we have tried 2 contact u...\n",
       "5568                Will Ì_ b going to esplanade fr home?\n",
       "5569    Pity, * was in mood for that. So...any other s...\n",
       "5570    The guy did some bitching but I acted like i'd...\n",
       "5571                           Rofl. Its true to its name\n",
       "Name: text, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words=\"english\", max_df=0.1, max_features=5000)\n",
    "X = count.fit_transform(dataset[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/Data/Projects/PML/venv/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=1, n_topics=5, perp_tol=0.1,\n",
       "             random_state=1, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=5, random_state=1, learning_method=\"batch\")\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_topics = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "know love day good dont just great like send life\n",
      "Topic 1\n",
      "ok going home want yes don work just da doing\n",
      "Topic 2\n",
      "free txt ur stop reply mobile text send just www\n",
      "Topic 3\n",
      "gt lt ll sorry later ì_ lor like got come\n",
      "Topic 4\n",
      "ur come night time good just today meet got ll\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d\" % (topic_idx))\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"topic\"] = X_topics.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_counter(label):\n",
    "    def counter(values):\n",
    "        return (values == label).sum()\n",
    "    counter.__name__ = label\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stats = dataset[[\"label\", \"topic\"]]\\\n",
    "    .groupby(by=\"topic\")\\\n",
    "    .agg([\"count\", label_counter(\"spam\"), label_counter(\"ham\")])\\\n",
    "    .rename(columns={\"count\": \"N\"})[\"label\"]\n",
    "topic_stats[\"spam_ratio\"] = topic_stats[\"spam\"]/topic_stats[\"N\"]\n",
    "topic_stats[\"ham_ratio\"] = topic_stats[\"ham\"]/topic_stats[\"N\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>spam</th>\n",
       "      <th>ham</th>\n",
       "      <th>spam_ratio</th>\n",
       "      <th>ham_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1323</td>\n",
       "      <td>47</td>\n",
       "      <td>1276</td>\n",
       "      <td>0.035525</td>\n",
       "      <td>0.964475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1153</td>\n",
       "      <td>14</td>\n",
       "      <td>1139</td>\n",
       "      <td>0.012142</td>\n",
       "      <td>0.987858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>911</td>\n",
       "      <td>500</td>\n",
       "      <td>411</td>\n",
       "      <td>0.548847</td>\n",
       "      <td>0.451153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1103</td>\n",
       "      <td>97</td>\n",
       "      <td>1006</td>\n",
       "      <td>0.087942</td>\n",
       "      <td>0.912058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1082</td>\n",
       "      <td>89</td>\n",
       "      <td>993</td>\n",
       "      <td>0.082255</td>\n",
       "      <td>0.917745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          N  spam   ham  spam_ratio  ham_ratio\n",
       "topic                                         \n",
       "0      1323    47  1276    0.035525   0.964475\n",
       "1      1153    14  1139    0.012142   0.987858\n",
       "2       911   500   411    0.548847   0.451153\n",
       "3      1103    97  1006    0.087942   0.912058\n",
       "4      1082    89   993    0.082255   0.917745"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
